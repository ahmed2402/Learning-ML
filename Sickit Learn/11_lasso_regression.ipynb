{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57155f9",
   "metadata": {},
   "source": [
    "Lasso Regression, which stands for `Least Absolute Shrinkage and Selection Operator`, is a type of linear regression that uses shrinkage. Shrinkage here means that the data values are shrunk towards a central point, like the mean. The lasso technique encourages simple, sparse models (i.e., models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "### Key Features of Lasso Regression:\n",
    "\n",
    "1. **Regularization Term**: The key characteristic of Lasso Regression is that it adds an L1 penalty to the regression model, which is the absolute value of the magnitude of the coefficients. The cost function for Lasso regression is:\n",
    "\n",
    "   $$ \\text{Minimize } \\sum_{i=1}^{n} (y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n",
    "\n",
    "   where $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "2. **Feature Selection**: One of the advantages of lasso regression over ridge regression is that it can result in sparse models with few coefficients; some coefficients can become exactly zero and be eliminated from the model. This property is called automatic feature selection and is a form of embedded method.\n",
    "\n",
    "3. **Parameter Tuning**: The strength of the L1 penalty is determined by a parameter, typically denoted as alpha or lambda. Selecting a good value for this parameter is crucial and is typically done using cross-validation.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**: Similar to ridge regression, lasso also manages the bias-variance tradeoff in model training. Increasing the regularization strength increases bias but decreases variance, potentially leading to better generalization on unseen data.\n",
    "\n",
    "5. **Scaling**: Before applying lasso, it is recommended to scale/normalize the data as lasso is sensitive to the scale of input features.\n",
    "\n",
    "### Implementation in Scikit-Learn:\n",
    "\n",
    "Lasso regression can be implemented using the `Lasso` class from Scikit-Learn's `linear_model` module. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ae1e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.011183765115093535\n",
      "Lasso Regression MSE: 0.3847492638484498\n",
      "Ridge Regression MSE: 0.05090866185225746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X,y = make_regression(random_state=42, n_samples=1000, n_features=15 , noise=0.1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lasso = Lasso(alpha=0.2)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lr_pred = lr.predict(X_test)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "\n",
    "print(\"Linear Regression MSE:\", mean_squared_error(y_test, lr_pred))\n",
    "print(\"Lasso Regression MSE:\", mean_squared_error(y_test, lasso_pred))\n",
    "print(\"Ridge Regression MSE:\", mean_squared_error(y_test, ridge_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbb212",
   "metadata": {},
   "source": [
    "In this example, `alpha` is the parameter that controls the amount of L1 regularization applied to the model. Fine-tuning `alpha` through techniques like cross-validation is a common practice to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebf69c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Lasso Regression: {'alpha': 1.0}\n",
      "Best score for Lasso Regression: 0.9995945087010266\n",
      "Best alpha for Ridge Regression: {'alpha': 1.0}\n",
      "Best score for Ridge Regression: 0.9999974601713119\n",
      "CPU times: total: 7.59 s\n",
      "Wall time: 34.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fine tune alpha value using cv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha' : np.arange(1,10,0.01)}\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=10, n_jobs=-1)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha for Lasso Regression:\", lasso_cv.best_params_)\n",
    "print(\"Best score for Lasso Regression:\", lasso_cv.best_score_)\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=10, n_jobs=-1)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha for Ridge Regression:\", ridge_cv.best_params_)\n",
    "print(\"Best score for Ridge Regression:\", ridge_cv.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7d427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Lasso Regression: {'alpha': 1.4200000000000004}\n",
      "Best score for Lasso Regression: 0.9991907614175057\n",
      "Best alpha for Ridge Regression: {'alpha': 1.2400000000000002}\n",
      "Best score for Ridge Regression: 0.999996308187975\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 474 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fine tune alpha value using cv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {'alpha' : np.arange(1,10,0.01)}\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "lasso_cv = RandomizedSearchCV(lasso, param_grid, cv=10, n_jobs=-1)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha for Lasso Regression:\", lasso_cv.best_params_)\n",
    "print(\"Best score for Lasso Regression:\", lasso_cv.best_score_)\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge_cv = RandomizedSearchCV(ridge, param_grid, cv=10, n_jobs=-1)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha for Ridge Regression:\", ridge_cv.best_params_)\n",
    "print(\"Best score for Ridge Regression:\", ridge_cv.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd8d33",
   "metadata": {},
   "source": [
    "### **Lasso Regression Explained Simply**  \n",
    "\n",
    "#### **What is Lasso Regression?**  \n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regression is a modified version of linear regression that helps prevent **overfitting** and can **automatically select important features** by shrinking some coefficients to **zero**.  \n",
    "\n",
    "### **Key Idea:**  \n",
    "- Like Ridge Regression, Lasso adds a penalty to the model's coefficients to keep them small.  \n",
    "- But unlike Ridge (which only shrinks coefficients), Lasso can **completely eliminate** unimportant features by setting their coefficients to **zero**.  \n",
    "- This makes Lasso useful for **feature selection**‚Äîhelping you identify which variables actually matter.  \n",
    "\n",
    "### **Why Use Lasso?**  \n",
    "‚úî **Reduces overfitting** (makes the model simpler).  \n",
    "‚úî **Automatically selects important features** (good when you have many useless/redundant features).  \n",
    "‚úî Works well when only a few features actually impact the outcome.  \n",
    "\n",
    "### **How Does It Work?**  \n",
    "Lasso modifies the usual linear regression cost function by adding a penalty:  \n",
    "\n",
    "\\[\n",
    "\\text{Cost} = \\text{Sum of Squared Errors} + \\lambda \\times (\\text{Sum of Absolute Coefficients})\n",
    "\\]  \n",
    "\n",
    "- **Œª (lambda)**: Controls penalty strength (higher Œª ‚Üí more coefficients become zero).  \n",
    "- Uses **absolute values** (L1 penalty) instead of squared values (like Ridge).  \n",
    "\n",
    "### **Example:**  \n",
    "Imagine predicting house prices using features like:  \n",
    "- **Size** (important)  \n",
    "- **Number of bedrooms** (somewhat important)  \n",
    "- **Wall color** (useless)  \n",
    "\n",
    "Lasso might:  \n",
    "‚úÖ Keep **Size** (big coefficient)  \n",
    "‚úÖ Slightly shrink **Bedrooms** (small coefficient)  \n",
    "‚ùå Eliminate **Wall color** (coefficient = 0)  \n",
    "\n",
    "### **Lasso vs. Ridge:**  \n",
    "| **Lasso (L1)** | **Ridge (L2)** |  \n",
    "|--------------|--------------|  \n",
    "| Shrinks some coefficients to **zero** (feature selection) | Shrinks coefficients but **never to zero** |  \n",
    "| Good when **few features matter** | Good when **many features contribute** |  \n",
    "| Uses **absolute values** in penalty | Uses **squared values** in penalty |  \n",
    "\n",
    "### **When to Use Lasso?**  \n",
    "- You have **many features** and suspect only a few are useful.  \n",
    "- You want **automatic feature selection**.  \n",
    "- Your data has **some useless/redundant features**.  \n",
    "\n",
    "### **Final Thought:**  \n",
    "Lasso is like a \"smart filter\" for your regression model‚Äîit keeps what‚Äôs important and discards the rest! üéØ  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
