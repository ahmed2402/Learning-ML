{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9093c94f",
   "metadata": {},
   "source": [
    "Ridge regression, also known as Tikhonov regularization, is a type of linear regression that includes a regularization term. The key idea behind ridge regression is to find a new line that doesn't fit the training data as well as ordinary least squares regression, in order to achieve better generalization to new data. This is particularly useful when dealing with multicollinearity (independent variables are highly correlated) or when the number of predictors (features) exceeds the number of observations.\n",
    "\n",
    "### Key Concept:\n",
    "- **Regularization**: Ridge regression adds a penalty equal to the square of the magnitude of coefficients. This penalty term (squared L2 norm) shrinks the coefficients towards zero, but it doesn't make them exactly zero.\n",
    "\n",
    "### Mathematical Representation:\n",
    "The ridge regression modifies the least squares objective function by adding a penalty term:\n",
    "\n",
    "$$ \\text{Minimize } \\sum_{i=1}^{n} (y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 $$\n",
    "\n",
    "where:\n",
    "- $ y_i $ is the response value for the ith observation.\n",
    "- $ x_{ij} $ is the value of the jth predictor for the ith observation.\n",
    "- $ \\beta_j $ is the regression coefficient for the jth predictor.\n",
    "- $ \\lambda $ is the tuning parameter that controls the strength of the penalty; $ \\lambda \\geq 0 $.\n",
    "\n",
    "\n",
    "In this code, `alpha` is the regularization strength \\( \\lambda \\). Adjusting `alpha` changes the strength of the regularization penalty. A larger `alpha` enforces stronger regularization (leading to smaller coefficients), and a smaller `alpha` tends towards a model similar to linear regression.\n",
    "\n",
    "### Key Points:\n",
    "- **Choosing Alpha**: Selecting the right value of `alpha` is crucial. It can be done using cross-validation techniques like `RidgeCV`.\n",
    "- **Standardization**: It's often recommended to standardize the predictors before applying ridge regression.\n",
    "- **Bias-Variance Tradeoff**: Ridge regression balances the bias-variance tradeoff in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5161ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [1. 2.]\n",
      "Intercept: 3.0000000000000018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# Target values\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "\n",
    "# Linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", lr.coef_)\n",
    "# Intercept\n",
    "print(\"Intercept:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d7468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Coefficients: [0.90909091 1.63636364]\n",
      "Ridge Intercept: 3.8636363636363633\n"
     ]
    }
   ],
   "source": [
    "#ridge regression\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)\n",
    "print(\"Ridge Intercept:\", ridge.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4becce",
   "metadata": {},
   "source": [
    "# Comparing Simple Linear Regression vs. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d477382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the data set\n",
    "df = sns.load_dataset('diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7c8950b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carat      0\n",
       "cut        0\n",
       "color      0\n",
       "clarity    0\n",
       "depth      0\n",
       "table      0\n",
       "price      0\n",
       "x          0\n",
       "y          0\n",
       "z          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['z'] = df['z'].fillna(df['z'].median())\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "734877d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# numeric features\n",
    "numeric_features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "# categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat',OneHotEncoder(),categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "588f24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor',preprocessor),\n",
    "        ('regressor',LinearRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "ridge_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor',preprocessor),\n",
    "        ('regressor',Ridge(alpha=1.0))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9963b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 1641246.9824469788\n",
      "Ridge Regression MSE: 1641198.7640876612\n",
      "------------------------\n",
      "Linear Regression R2: 0.9132764312325968\n",
      "Ridge Regression R2: 0.913278979093001\n",
      "------------------------\n",
      "Linear Regression MAE: 882.4325038403573\n",
      "Ridge Regression MAE: 882.4060964639548\n",
      "------------------------\n",
      "Linear Regression MAPE: 0.3913935361894944\n",
      "Ridge Regression MAPE: 0.3911448808103667\n",
      "------------------------\n",
      "Linear Regression RMSE: 1281.111619823573\n",
      "Ridge Regression RMSE: 1281.0928007321177\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Linear Regression\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "lr_pred = lr_pipeline.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "lr_r2 = r2_score(y_test, lr_pred)\n",
    "lr_mae = mean_absolute_error(y_test, lr_pred)\n",
    "lr_mape = mean_absolute_percentage_error(y_test, lr_pred)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "\n",
    "# Train and evaluate Ridge Regression\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "ridge_pred = ridge_pipeline.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "ridhe_r2 = r2_score(y_test, ridge_pred)\n",
    "ridge_mae = mean_absolute_error(y_test, ridge_pred)\n",
    "ridge_mape = mean_absolute_percentage_error(y_test, ridge_pred)\n",
    "ridge_rmse = np.sqrt(ridge_mse)\n",
    "\n",
    "print(\"Linear Regression MSE:\", lr_mse)\n",
    "print(\"Ridge Regression MSE:\", ridge_mse)\n",
    "print(f\"------------------------\")\n",
    "\n",
    "print(\"Linear Regression R2:\", lr_r2)\n",
    "print(\"Ridge Regression R2:\", ridhe_r2)\n",
    "print(f\"------------------------\")\n",
    "print(\"Linear Regression MAE:\", lr_mae)\n",
    "print(\"Ridge Regression MAE:\", ridge_mae)\n",
    "print(f\"------------------------\")\n",
    "print(\"Linear Regression MAPE:\", lr_mape)\n",
    "print(\"Ridge Regression MAPE:\", ridge_mape)\n",
    "print(f\"------------------------\")\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
